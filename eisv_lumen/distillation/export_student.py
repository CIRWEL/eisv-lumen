"""Export student classifiers to JSON for zero-dependency Pi deployment.

Extracts decision tree rules from RandomForest classifiers and exports
them as JSON. Generates a standalone inference module that runs with
only stdlib + basic math (no numpy, no sklearn).

Usage:
    python -m eisv_lumen.distillation.export_student \
        --models outputs/student \
        --output outputs/student/exported
"""

from __future__ import annotations

import json
import os
from typing import Any, Dict, List

import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler

from eisv_lumen.distillation.train_student import (
    StudentModels,
    load_student_models,
    NUMERIC_FEATURES,
    SHAPE_NAMES,
    ALL_TOKENS,
    ALL_PATTERNS,
)


def _export_tree(tree, feature_names: List[str]) -> Dict[str, Any]:
    """Export a single decision tree to a JSON-serializable dict.

    Each node is: {feature, threshold, left, right} for splits,
    or {probs: [...]} for leaves (normalized class probabilities).

    We export probability distributions (not just majority class) because
    sklearn's RandomForest.predict() averages probabilities across trees,
    not majority vote.
    """
    tree_ = tree.tree_

    def _recurse(node_id: int) -> Dict[str, Any]:
        if tree_.children_left[node_id] == tree_.children_right[node_id]:
            # Leaf node — export normalized class probabilities
            counts = tree_.value[node_id][0]
            total = counts.sum()
            probs = (counts / total).tolist() if total > 0 else counts.tolist()
            return {"leaf": True, "probs": probs}
        else:
            feature_idx = int(tree_.feature[node_id])
            threshold = float(tree_.threshold[node_id])
            return {
                "leaf": False,
                "feature": feature_idx,
                "threshold": threshold,
                "left": _recurse(int(tree_.children_left[node_id])),
                "right": _recurse(int(tree_.children_right[node_id])),
            }

    return _recurse(0)


def export_forest(clf: RandomForestClassifier) -> List[Dict[str, Any]]:
    """Export all trees in a RandomForest."""
    return [_export_tree(est, []) for est in clf.estimators_]


def export_scaler(scaler: StandardScaler) -> Dict[str, List[float]]:
    """Export StandardScaler parameters."""
    return {
        "mean": scaler.mean_.tolist(),
        "scale": scaler.scale_.tolist(),
    }


def export_student_to_json(models: StudentModels, output_dir: str) -> None:
    """Export all student models to JSON files."""
    os.makedirs(output_dir, exist_ok=True)

    # Export forests
    for name, clf in [
        ("pattern_forest", models.pattern_clf),
        ("token1_forest", models.token1_clf),
        ("token2_forest", models.token2_clf),
    ]:
        trees = export_forest(clf)
        path = os.path.join(output_dir, f"{name}.json")
        with open(path, "w") as f:
            json.dump(trees, f)
        size_kb = os.path.getsize(path) / 1024
        print(f"  {name}.json: {size_kb:.1f} KB ({len(trees)} trees)")

    # Export scaler
    scaler_data = export_scaler(models.scaler)
    path = os.path.join(output_dir, "scaler.json")
    with open(path, "w") as f:
        json.dump(scaler_data, f)

    # Export class mappings — MUST use encoder order, not hardcoded order!
    # The forest class_index values correspond to LabelEncoder's sorted classes.
    mappings = {
        "shapes": list(models.shape_encoder.classes_),
        "patterns": list(models.pattern_encoder.classes_),
        "tokens": list(models.token1_encoder.classes_),
        "tokens_with_none": list(models.token2_encoder.classes_),
        "numeric_features": NUMERIC_FEATURES,
    }
    path = os.path.join(output_dir, "mappings.json")
    with open(path, "w") as f:
        json.dump(mappings, f, indent=2)

    # Report total size
    total = sum(
        os.path.getsize(os.path.join(output_dir, f))
        for f in os.listdir(output_dir) if f.endswith(".json")
    )
    print(f"\n  Total JSON export: {total / 1024:.1f} KB")


def generate_inference_module(output_dir: str) -> None:
    """Generate standalone Python inference module.

    Produces student_inference.py that can run with only stdlib.
    No numpy, no sklearn needed.
    """
    module_code = '''"""Standalone student model inference — zero external dependencies.

Generated by eisv_lumen.distillation.export_student.
Loads JSON-exported RandomForest classifiers and runs inference
using only Python stdlib.

Usage:
    from student_inference import StudentInference

    student = StudentInference("path/to/exported/")
    result = student.predict("settled_presence", {
        "mean_E": 0.7, "mean_I": 0.6, "mean_S": 0.2, "mean_V": 0.05,
        "dE": 0.0, "dI": 0.0, "dS": 0.0, "dV": 0.0,
        "d2E": 0.0, "d2I": 0.0, "d2S": 0.0, "d2V": 0.0,
    })
    # result = {"pattern": "SINGLE", "eisv_tokens": ["~stillness~"]}
"""

from __future__ import annotations

import json
import os
from typing import Any, Dict, List, Optional


class StudentInference:
    """Zero-dependency student model inference."""

    def __init__(self, model_dir: str):
        self._model_dir = model_dir
        self._load_models()

    def _load_models(self) -> None:
        """Load JSON-exported models."""
        def _load(name: str):
            path = os.path.join(self._model_dir, name)
            with open(path) as f:
                return json.load(f)

        self._pattern_forest = _load("pattern_forest.json")
        self._token1_forest = _load("token1_forest.json")
        self._token2_forest = _load("token2_forest.json")
        self._scaler = _load("scaler.json")
        self._mappings = _load("mappings.json")

    def _scale_features(self, numeric: List[float]) -> List[float]:
        """Apply StandardScaler normalization."""
        mean = self._scaler["mean"]
        scale = self._scaler["scale"]
        return [(v - m) / s for v, m, s in zip(numeric, mean, scale)]

    def _build_features(self, shape: str, features: Dict[str, float]) -> List[float]:
        """Build feature vector from shape + numeric features."""
        # Numeric features (scaled)
        numeric = [features.get(f, 0.0) for f in self._mappings["numeric_features"]]
        scaled = self._scale_features(numeric)

        # Shape one-hot
        shapes = self._mappings["shapes"]
        shape_onehot = [1.0 if s == shape else 0.0 for s in shapes]

        return scaled + shape_onehot

    def _predict_tree(self, tree: Dict, features: List[float]) -> List[float]:
        """Walk a single decision tree to get class probabilities."""
        node = tree
        while not node.get("leaf", False):
            feat_idx = node["feature"]
            threshold = node["threshold"]
            if features[feat_idx] <= threshold:
                node = node["left"]
            else:
                node = node["right"]
        return node["probs"]

    def _predict_forest(self, forest: List[Dict], features: List[float]) -> int:
        """Average class probabilities across all trees (matches sklearn)."""
        all_probs = [self._predict_tree(tree, features) for tree in forest]
        n_classes = len(all_probs[0])
        avg = [0.0] * n_classes
        for probs in all_probs:
            for i in range(n_classes):
                avg[i] += probs[i]
        # argmax of averaged probabilities
        best_idx = 0
        best_val = avg[0]
        for i in range(1, n_classes):
            if avg[i] > best_val:
                best_val = avg[i]
                best_idx = i
        return best_idx

    def predict(
        self,
        shape: str,
        features: Dict[str, float],
    ) -> Dict[str, Any]:
        """Run student inference.

        Parameters
        ----------
        shape : str
            Trajectory shape name.
        features : dict
            Numeric features with keys: mean_E, mean_I, mean_S, mean_V,
            dE, dI, dS, dV, d2E, d2I, d2S, d2V.

        Returns
        -------
        dict with keys: pattern, eisv_tokens, token_1, token_2.
        """
        X = self._build_features(shape, features)

        # Predict pattern
        pattern_idx = self._predict_forest(self._pattern_forest, X)
        pattern = self._mappings["patterns"][pattern_idx]

        # Predict token-1
        token1_idx = self._predict_forest(self._token1_forest, X)
        token_1 = self._mappings["tokens"][token1_idx]

        # Predict token-2 (add token_1 index as extra feature)
        X_t2 = X + [float(token1_idx)]
        token2_idx = self._predict_forest(self._token2_forest, X_t2)
        token_2 = self._mappings["tokens_with_none"][token2_idx]

        # Build token list based on pattern
        if pattern == "SINGLE":
            eisv_tokens = [token_1]
        elif pattern == "REPETITION":
            eisv_tokens = [token_1, token_1]
        elif pattern in ("PAIR", "QUESTION"):
            eisv_tokens = [token_1, token_2] if token_2 != "none" else [token_1]
        elif pattern == "TRIPLE":
            eisv_tokens = [token_1, token_2] if token_2 != "none" else [token_1]
        else:
            eisv_tokens = [token_1]

        return {
            "pattern": pattern,
            "token_1": token_1,
            "token_2": token_2,
            "eisv_tokens": eisv_tokens,
        }
'''

    path = os.path.join(output_dir, "student_inference.py")
    with open(path, "w") as f:
        f.write(module_code)
    print(f"  Generated {path}")


def verify_export(models: StudentModels, output_dir: str, n_tests: int = 100) -> bool:
    """Verify that JSON export produces identical results to sklearn models.

    Runs n_tests random examples through both the sklearn models and the
    JSON-exported inference, checking for exact agreement.
    """
    # Import the generated module
    import importlib.util
    spec = importlib.util.spec_from_file_location(
        "student_inference",
        os.path.join(output_dir, "student_inference.py"),
    )
    mod = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(mod)

    student = mod.StudentInference(output_dir)

    # Generate random test inputs
    rng = np.random.RandomState(42)
    mismatches = 0

    for i in range(n_tests):
        shape = SHAPE_NAMES[rng.randint(0, len(SHAPE_NAMES))]
        features = {f: float(rng.uniform(-1, 1)) for f in NUMERIC_FEATURES}

        # sklearn prediction
        from eisv_lumen.distillation.train_student import predict as sklearn_predict
        sklearn_result = sklearn_predict(models, shape, features)

        # JSON prediction
        json_result = student.predict(shape, features)

        if sklearn_result["token_1"] != json_result["token_1"]:
            mismatches += 1
        if sklearn_result["pattern"] != json_result["pattern"]:
            mismatches += 1

    match_rate = 1.0 - (mismatches / (n_tests * 2))
    print(f"\n  Export verification: {match_rate:.1%} agreement ({mismatches} mismatches in {n_tests * 2} checks)")

    return mismatches == 0


def main():
    import argparse

    parser = argparse.ArgumentParser(description="Export student models to JSON")
    parser.add_argument(
        "--models", required=True,
        help="Path to student model directory (pkl files)",
    )
    parser.add_argument(
        "--output", default=None,
        help="Output directory for JSON files (default: {models}/exported)",
    )
    parser.add_argument(
        "--verify", action="store_true",
        help="Run export verification",
    )
    args = parser.parse_args()

    output_dir = args.output or os.path.join(args.models, "exported")

    # Load sklearn models
    print(f"Loading models from {args.models} ...")
    models = load_student_models(args.models)

    # Export to JSON
    print(f"\nExporting to {output_dir} ...")
    export_student_to_json(models, output_dir)

    # Generate inference module
    generate_inference_module(output_dir)

    # Verify
    if args.verify:
        print("\nVerifying export ...")
        ok = verify_export(models, output_dir)
        if ok:
            print("  ✓ Export verified — JSON inference matches sklearn exactly")
        else:
            print("  ✗ WARNING: Export mismatch detected!")


if __name__ == "__main__":
    main()
