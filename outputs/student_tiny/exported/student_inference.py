"""Standalone student model inference â€” zero external dependencies.

Generated by eisv_lumen.distillation.export_student.
Loads JSON-exported RandomForest classifiers and runs inference
using only Python stdlib.

Usage:
    from student_inference import StudentInference

    student = StudentInference("path/to/exported/")
    result = student.predict("settled_presence", {
        "mean_E": 0.7, "mean_I": 0.6, "mean_S": 0.2, "mean_V": 0.05,
        "dE": 0.0, "dI": 0.0, "dS": 0.0, "dV": 0.0,
        "d2E": 0.0, "d2I": 0.0, "d2S": 0.0, "d2V": 0.0,
    })
    # result = {"pattern": "SINGLE", "eisv_tokens": ["~stillness~"]}
"""

from __future__ import annotations

import json
import os
from typing import Any, Dict, List, Optional


class StudentInference:
    """Zero-dependency student model inference."""

    def __init__(self, model_dir: str):
        self._model_dir = model_dir
        self._load_models()

    def _load_models(self) -> None:
        """Load JSON-exported models."""
        def _load(name: str):
            path = os.path.join(self._model_dir, name)
            with open(path) as f:
                return json.load(f)

        self._pattern_forest = _load("pattern_forest.json")
        self._token1_forest = _load("token1_forest.json")
        self._token2_forest = _load("token2_forest.json")
        self._scaler = _load("scaler.json")
        self._mappings = _load("mappings.json")

    def _scale_features(self, numeric: List[float]) -> List[float]:
        """Apply StandardScaler normalization."""
        mean = self._scaler["mean"]
        scale = self._scaler["scale"]
        return [(v - m) / s for v, m, s in zip(numeric, mean, scale)]

    def _build_features(self, shape: str, features: Dict[str, float]) -> List[float]:
        """Build feature vector from shape + numeric features."""
        # Numeric features (scaled)
        numeric = [features.get(f, 0.0) for f in self._mappings["numeric_features"]]
        scaled = self._scale_features(numeric)

        # Shape one-hot
        shapes = self._mappings["shapes"]
        shape_onehot = [1.0 if s == shape else 0.0 for s in shapes]

        return scaled + shape_onehot

    def _predict_tree(self, tree: Dict, features: List[float]) -> List[float]:
        """Walk a single decision tree to get class probabilities."""
        node = tree
        while not node.get("leaf", False):
            feat_idx = node["feature"]
            threshold = node["threshold"]
            if features[feat_idx] <= threshold:
                node = node["left"]
            else:
                node = node["right"]
        return node["probs"]

    def _predict_forest(self, forest: List[Dict], features: List[float]) -> int:
        """Average class probabilities across all trees (matches sklearn)."""
        all_probs = [self._predict_tree(tree, features) for tree in forest]
        n_classes = len(all_probs[0])
        avg = [0.0] * n_classes
        for probs in all_probs:
            for i in range(n_classes):
                avg[i] += probs[i]
        # argmax of averaged probabilities
        best_idx = 0
        best_val = avg[0]
        for i in range(1, n_classes):
            if avg[i] > best_val:
                best_val = avg[i]
                best_idx = i
        return best_idx

    def predict(
        self,
        shape: str,
        features: Dict[str, float],
    ) -> Dict[str, Any]:
        """Run student inference.

        Parameters
        ----------
        shape : str
            Trajectory shape name.
        features : dict
            Numeric features with keys: mean_E, mean_I, mean_S, mean_V,
            dE, dI, dS, dV, d2E, d2I, d2S, d2V.

        Returns
        -------
        dict with keys: pattern, eisv_tokens, token_1, token_2.
        """
        X = self._build_features(shape, features)

        # Predict pattern
        pattern_idx = self._predict_forest(self._pattern_forest, X)
        pattern = self._mappings["patterns"][pattern_idx]

        # Predict token-1
        token1_idx = self._predict_forest(self._token1_forest, X)
        token_1 = self._mappings["tokens"][token1_idx]

        # Predict token-2 (add token_1 index as extra feature)
        X_t2 = X + [float(token1_idx)]
        token2_idx = self._predict_forest(self._token2_forest, X_t2)
        token_2 = self._mappings["tokens_with_none"][token2_idx]

        # Build token list based on pattern
        if pattern == "SINGLE":
            eisv_tokens = [token_1]
        elif pattern == "REPETITION":
            eisv_tokens = [token_1, token_1]
        elif pattern in ("PAIR", "QUESTION"):
            eisv_tokens = [token_1, token_2] if token_2 != "none" else [token_1]
        elif pattern == "TRIPLE":
            eisv_tokens = [token_1, token_2] if token_2 != "none" else [token_1]
        else:
            eisv_tokens = [token_1]

        return {
            "pattern": pattern,
            "token_1": token_1,
            "token_2": token_2,
            "eisv_tokens": eisv_tokens,
        }
